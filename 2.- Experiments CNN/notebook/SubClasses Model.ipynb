{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Input, Concatenate, Conv1D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, os, random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Files Setting\n",
    "limit = 8000 # Maximum amount of Star Per Class Per Survey\n",
    "extraRandom = True\n",
    "permutation = True # Permute Files\n",
    "BALANCE_DB = True # Balance or not\n",
    "\n",
    "# Mini Settings\n",
    "MAX_NUMBER_OF_POINTS = 500\n",
    "NUMBER_OF_POINTS = 500\n",
    "n_splits = 10\n",
    "validation_set = 0.15\n",
    "\n",
    "# Iterations\n",
    "step = 250\n",
    "stepForDebug = 300\n",
    "\n",
    "# Network Settings\n",
    "verbose = True\n",
    "batch_size = 256\n",
    "dropout = 0.5\n",
    "hidden_dims = 128\n",
    "epochs = 10 # 850\n",
    "\n",
    "# Convolutions\n",
    "filters = 128\n",
    "filters2 = 64\n",
    "kernel_size = 50\n",
    "kernel_size2 = 50\n",
    "\n",
    "# Paths\n",
    "NombreCarpeta = ''\n",
    "base_path = '/Users/Carlos/Desktop/Magister/2*/'\n",
    "regular_exp = base_path + 'Data/Corot/**/*.csv'\n",
    "regular_exp2 = base_path + 'Data/**/OGLE-*.dat'\n",
    "regular_exp3 = base_path + 'Data/VVV/**/*.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Open Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Methods\n",
    "# subclasses = ['cepDiez', 'cepEfe', 'RRab', 'RRc', 'nonEC', 'EC', 'Mira', 'SRV', 'Osarg'] \n",
    "subclasses = ['lpv','cep','rrlyr','ecl']\n",
    "\n",
    "def get_filename(directory, N, early, activation='relu'):\n",
    "    if activation == 'relu':\n",
    "        directory += '/relu/'\n",
    "    elif activation == 'sigmoid':\n",
    "        directory += '/sigmoid/'\n",
    "    else:\n",
    "        directory += '/tanh/'\n",
    "        \n",
    "    if not os.path.exists(directory):\n",
    "        print('[+] Creando Directorio \\n\\t ->', directory)\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "    name = '1) Red ' + str(N)\n",
    "    directory += '/'\n",
    "    return directory, name\n",
    "\n",
    "def get_files(extraRandom = False, permutation=False):\n",
    "    files1 = np.array(list(glob.iglob(regular_exp, recursive=True)))\n",
    "    files2 = np.array(list(glob.iglob(regular_exp2, recursive=True)))    \n",
    "    files3 = np.array(list(glob.iglob(regular_exp3, recursive=True)))\n",
    "    \n",
    "    print('[!] Files in Memory')\n",
    "    \n",
    "    # Permutations\n",
    "    if permutation:\n",
    "        files1 = files1[np.random.permutation(len(files1))]\n",
    "        files2 = files2[np.random.permutation(len(files2))]\n",
    "        files3 = files3[np.random.permutation(len(files3))]\n",
    "        \n",
    "        print('[!] Permutation applied')\n",
    "        \n",
    "    aux_dic = {}\n",
    "    corot = {}\n",
    "    vvv = {}\n",
    "    ogle = {}\n",
    "    for subclass in subclasses:\n",
    "        aux_dic[subclass] = []\n",
    "        corot[subclass] = 0\n",
    "        vvv[subclass] = 0\n",
    "        ogle[subclass] = 0\n",
    "\n",
    "        \n",
    "    new_files = []\n",
    "    for idx in tqdm(range(len(files2))):\n",
    "        foundCorot = False\n",
    "        foundVista = False\n",
    "        foundOgle = False\n",
    "        \n",
    "        for subclass in subclasses:        \n",
    "            # Corot\n",
    "            if not foundCorot and corot[subclass] < limit and idx < len(files1) and subclass in files1[idx]:\n",
    "                new_files += [[files1[idx], 0]]\n",
    "                corot[subclass] += 1\n",
    "                foundCorot = True\n",
    "                    \n",
    "            # Ogle\n",
    "            if not foundOgle and ogle[subclass] < limit and subclass in files2[idx]:\n",
    "                new_files += [[files2[idx], 0]]\n",
    "                ogle[subclass] += 1\n",
    "                foundOgle = True            \n",
    "                    \n",
    "            # VVV           \n",
    "            if not foundVista and vvv[subclass] < limit and idx < len(files3) and subclass in files3[idx]:\n",
    "                new_files += [[files3[idx], 0]]\n",
    "                vvv[subclass] += 1\n",
    "                foundVista = True   \n",
    "    \n",
    "    del files1, files2, files3\n",
    "\n",
    "    print('[!] Loaded Files')\n",
    "    \n",
    "    return new_files\n",
    "\n",
    "\n",
    "def replicate_by_survey(files, yTrain):\n",
    "        \n",
    "    surveys = [\"OGLE\", \"VVV\", \"Corot\"]\n",
    "    \n",
    "    new_files = []\n",
    "    for s in surveys:\n",
    "        mask = [ s in i for i in yTrain]\n",
    "        auxYTrain = yTrain[mask]\n",
    "            \n",
    "        new_files += replicate(files[mask])\n",
    "    \n",
    "    return new_files\n",
    "    \n",
    "\n",
    "def replicate(files):\n",
    "    aux_dic = {}\n",
    "    for subclass in subclasses:\n",
    "        aux_dic[subclass] = []\n",
    "    \n",
    "    for file, num in files:\n",
    "        for subclass in subclasses:\n",
    "            if subclass in file:\n",
    "                aux_dic[subclass].append([file, num])\n",
    "                break\n",
    "    \n",
    "    new_files = []\n",
    "    for subclass in subclasses:\n",
    "        array = aux_dic[subclass]\n",
    "        length = len(array)\n",
    "        if length == 0:\n",
    "            continue\n",
    "            \n",
    "        new_files += array\n",
    "        if length < limit and extraRandom:\n",
    "                count = 1\n",
    "                q = limit // length\n",
    "                for i in range(1, q):\n",
    "                    for file, num in array:\n",
    "                        new_files += [[file, count]]\n",
    "                    count += 1\n",
    "                r = limit - q*length\n",
    "                if r > 1:\n",
    "                    new_files += [[random.choice(array)[0], count] for i in range(r)]\n",
    "      \n",
    "    return new_files\n",
    "\n",
    "def get_survey(path):\n",
    "    if 'Corot' in path:\n",
    "        return 'Corot'\n",
    "    elif 'VVV' in path:\n",
    "        return 'VVV'\n",
    "    elif 'OGLE' in path:\n",
    "        return 'OGLE'\n",
    "    else:\n",
    "        return 'err'\n",
    "\n",
    "def get_name(path):\n",
    "    for subclass in subclasses:\n",
    "        if subclass in path:\n",
    "            return subclass\n",
    "    return 'err'\n",
    "\n",
    "def get_name_with_survey(path):\n",
    "    for subclass in subclasses:\n",
    "        if subclass in path:\n",
    "            survey = get_survey(path)\n",
    "            return survey + '_' + subclass\n",
    "    return 'err'\n",
    "\n",
    "def open_vista(path, num):\n",
    "    df = pd.read_csv(path, comment='#', sep=',')\n",
    "    df = df[df.mjd > 0]\n",
    "    df = df.sort_values(by=[df.columns[1]])\n",
    "\n",
    "    # 3 Desviaciones Standard\n",
    "    #df = df[np.abs(df.mjd-df.mjd.mean())<=(3*df.mjd.std())]\n",
    "\n",
    "    time = np.array(df[df.columns[1]].values, dtype=float)\n",
    "    magnitude = np.array(df[df.columns[2]].values, dtype=float)\n",
    "    error = np.array(df[df.columns[3]].values, dtype=float)\n",
    "\n",
    "    # Not Nan\n",
    "    not_nan = np.where(~np.logical_or(np.isnan(time), np.isnan(magnitude)))[0]\n",
    "    time = time[not_nan]\n",
    "    magnitude = magnitude[not_nan]\n",
    "    error = error[not_nan]\n",
    " \n",
    "    # Num\n",
    "    step = random.randint(1, 2)\n",
    "    count = random.randint(0, num)\n",
    "    \n",
    "    time = time[::step] \n",
    "    magnitude = magnitude[::step]\n",
    "    error = error[::step]\n",
    "    \n",
    "    time = time[count:] \n",
    "    magnitude = magnitude[count:]\n",
    "    error = error[count:]\n",
    "    \n",
    "    # Get Name of Class\n",
    "    # folder_path = os.path.dirname(os.path.dirname(os.path.dirname(path)))\n",
    "    # path, folder_name = os.path.split(folder_path)\n",
    "    \n",
    "    return time.astype('float'), magnitude.astype('float'), error.astype('float')\n",
    "\n",
    "def open_corot(path, num, n, columns):\n",
    "    df = pd.read_csv(path, comment='#', sep=',')\n",
    "    df = df[df.DATEBARTT > 0]\n",
    "    df = df.sort_values(by=[df.columns[columns[0]]])\n",
    "    \n",
    "    # 3 Desviaciones Standard\n",
    "    #df = df[np.abs(df.mjd-df.mjd.mean())<=(3*df.mjd.std())]\n",
    "    \n",
    "    time = np.array(df[df.columns[columns[0]]].values, dtype=float)\n",
    "    magnitude = np.array(df[df.columns[columns[1]]].values, dtype=float)\n",
    "    error = np.array(df[df.columns[columns[2]]].values, dtype=float)\n",
    "    \n",
    "    # Not Nan\n",
    "    not_nan = np.where(~np.logical_or(np.isnan(time), np.isnan(magnitude)))[0]\n",
    "    time = time[not_nan]\n",
    "    magnitude = magnitude[not_nan]\n",
    "    error = error[not_nan]\n",
    "    \n",
    "    # Num\n",
    "    step = random.randint(1, 2)\n",
    "    count = random.randint(0, num)\n",
    "    \n",
    "    time = time[::step] \n",
    "    magnitude = magnitude[::step]\n",
    "    error = error[::step]\n",
    "    \n",
    "    time = time[count:] \n",
    "    magnitude = magnitude[count:]\n",
    "    error = error[count:]\n",
    "    \n",
    "    if len(time) > n:\n",
    "        time = time[:n]\n",
    "        magnitude = magnitude[:n]\n",
    "        error = error[:n]\n",
    "        \n",
    "    # Get Name of Class\n",
    "    # folder_path = os.path.dirname(os.path.dirname(path))\n",
    "    # path, folder_name = os.path.split(folder_path)\n",
    "    \n",
    "    return time, magnitude, error\n",
    "\n",
    "def open_ogle(path, num, n, columns):\n",
    "    df = pd.read_csv(path, comment='#', sep='\\s+', header=None)\n",
    "    df.columns = ['a','b','c']\n",
    "    df = df[df.a > 0]\n",
    "    df = df.sort_values(by=[df.columns[columns[0]]])\n",
    "    \n",
    "    # Erase duplicates if it exist\n",
    "    df.drop_duplicates(subset='a', keep='first')\n",
    "    \n",
    "    # 3 Desviaciones Standard\n",
    "    #df = df[np.abs(df.mjd-df.mjd.mean())<=(3*df.mjd.std())]\n",
    "    \n",
    "    time = np.array(df[df.columns[columns[0]]].values, dtype=float)\n",
    "    magnitude = np.array(df[df.columns[columns[1]]].values, dtype=float)\n",
    "    error = np.array(df[df.columns[columns[2]]].values, dtype=float)\n",
    "    \n",
    "    # Not Nan\n",
    "    not_nan = np.where(~np.logical_or(np.isnan(time), np.isnan(magnitude)))[0]\n",
    "    time = time[not_nan]\n",
    "    magnitude = magnitude[not_nan]\n",
    "    error = error[not_nan]\n",
    "    \n",
    "    # Num\n",
    "    step = random.randint(1, 2)\n",
    "    count = random.randint(0, num)\n",
    "    \n",
    "    time = time[::step] \n",
    "    magnitude = magnitude[::step]\n",
    "    error = error[::step]\n",
    "    \n",
    "    time = time[count:] \n",
    "    magnitude = magnitude[count:]\n",
    "    error = error[count:]\n",
    "    \n",
    "    \n",
    "    if len(time) > n:\n",
    "        time = time[:n]\n",
    "        magnitude = magnitude[:n]\n",
    "        error = error[:n]\n",
    "        \n",
    "    # Get Name of Class\n",
    "    # folder_path = os.path.dirname(os.path.dirname(os.path.dirname(path)))\n",
    "    # path, folder_name = os.path.split(folder_path)\n",
    "    \n",
    "    return time, magnitude, error\n",
    "\n",
    "# Data has the form (Points,(Delta Time, Mag, Error)) 1D\n",
    "def create_matrix(data, N):\n",
    "    aux = np.append([0], np.diff(data).flatten())\n",
    "\n",
    "    # Padding with cero\n",
    "    if max(N-len(aux),0) > 0:\n",
    "        aux = np.append(aux, [0]*(N-len(aux)))    \n",
    "\n",
    "    return np.array(aux[:N], dtype='float').reshape(-1,1)\n",
    "    \n",
    "def dataset(files, N):\n",
    "    input_1 = []\n",
    "    input_2 = []\n",
    "    yClassTrain = []\n",
    "    survey = []\n",
    "    for file, num in tqdm(files):\n",
    "        num = int(num)\n",
    "        t, m, e, c, s = None, None, None, get_name(file), get_survey(file)\n",
    "        if c in subclasses:\n",
    "            if 'Corot' in file:\n",
    "                if 'EN2_STAR_CHR' in file:\n",
    "                    t, m, e = open_corot(file, num, N, [0,4,8])\n",
    "                else:\n",
    "                    t, m, e = open_corot(file, num, N, [0,1,2])\n",
    "            elif 'VVV' in file:\n",
    "                t, m, e = open_vista(file, num)\n",
    "            elif 'OGLE' in file:\n",
    "                t, m, e = open_ogle(file, num, N, [0,1,2])\n",
    "            if t != None and c in subclasses:\n",
    "                input_1.append(create_matrix(t, N))\n",
    "                input_2.append(create_matrix(m, N))\n",
    "                yClassTrain.append(c)\n",
    "                survey.append(s)\n",
    "            else:\n",
    "                print('\\t [!] E2 No paso el archivo: ', file, '\\n\\t\\t - Clase: ',  c)\n",
    "        else:\n",
    "            print('\\t [!] E1 No paso el archivo: ', file, '\\n\\t\\t - Clase: ',  c)\n",
    "    return np.array(input_1), np.array(input_2), np.array(yClassTrain), np.array(survey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_model(N, classes, activation='relu'):\n",
    "    conv1 = Conv1D(filters, kernel_size, activation='relu')\n",
    "    conv2 = Conv1D(filters2, kernel_size2, activation='relu')\n",
    "\n",
    "    # For Time Tower\n",
    "    input1 = Input((N, 1))\n",
    "    out1 = conv1(input1)\n",
    "    out1 = conv2(out1)\n",
    "\n",
    "    # For Magnitude Tower\n",
    "    input2 = Input((N, 1))\n",
    "    out2 = conv1(input2)\n",
    "    out2 = conv2(out2)\n",
    "\n",
    "    out = Concatenate()([out1, out2])\n",
    "    out = Flatten()(out)\n",
    "    out = Dropout(dropout)(out)\n",
    "    out = Dense(hidden_dims, activation=activation)(out)   \n",
    "    out = Dropout(dropout)(out)\n",
    "    out = Dense(len(classes), activation='softmax')(out)   \n",
    "\n",
    "    model = Model([input1, input2], out)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def class_to_vector(Y, classes):\n",
    "    new_y = []\n",
    "    for y in Y:\n",
    "        aux = []\n",
    "        for val in classes:\n",
    "            if val == y:\n",
    "                aux.append(1)\n",
    "            else:\n",
    "                aux.append(0)\n",
    "        new_y.append(aux)\n",
    "    return np.array(new_y)\n",
    "\n",
    "def serialize_model(name, model):\n",
    "    # Serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(name + '.json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "        \n",
    "    # Serialize weights to HDF5\n",
    "    model.save_weights(name + \".h5\")\n",
    "\n",
    "def experiment(directory, files, Y, classes, N, n_splits):\n",
    "    # Iterating\n",
    "    activations = ['tanh']\n",
    "    earlyStopping = [False]\n",
    "    \n",
    "    for early in earlyStopping:\n",
    "        for activation in activations:   \n",
    "#             try:\n",
    "            print('\\t\\t [+] Entrenando',\n",
    "                  '\\n\\t\\t\\t [!] Early Stopping', early,\n",
    "                  '\\n\\t\\t\\t [!] Activation', activation)\n",
    "\n",
    "\n",
    "            direc, name =  get_filename(directory, N, \n",
    "                                        early, activation) \n",
    "            filename_exp = direc + name\n",
    "            yPred = np.array([]) \n",
    "            yReal = np.array([]) \n",
    "            sReal = np.array([])\n",
    "\n",
    "            modelNum = 0\n",
    "            skf = StratifiedKFold(n_splits=n_splits)\n",
    "            for train_index, test_index in skf.split(files, Y):                           \n",
    "                dTrain, dTest = files[train_index], files[test_index]\n",
    "                yTrain = Y[train_index]\n",
    "\n",
    "                ##############\n",
    "                ### Get DB ###\n",
    "                ##############\n",
    "\n",
    "                # Replicate Files\n",
    "                dTrain = replicate_by_survey(dTrain, yTrain) \n",
    "\n",
    "                # Get Database\n",
    "                dTrain_1, dTrain_2, yTrain, _ = dataset(dTrain[::stepForDebug], N)\n",
    "                dTest_1, dTest_2, yTest, sTest  = dataset(dTest[::stepForDebug], N)\n",
    "\n",
    "                yReal = np.append(yReal, yTest)\n",
    "                sReal = np.append(sReal, sTest)\n",
    "                yTrain = class_to_vector(yTrain, classes)\n",
    "                yTest = class_to_vector(yTest, classes)\n",
    "\n",
    "                ################\n",
    "                ## Tensorboard #\n",
    "                ################\n",
    "\n",
    "                tensorboard = TensorBoard(log_dir= direc + 'logs', \n",
    "                                          write_graph=True, write_images=False)\n",
    "#                     tensorboard = TensorBoard(log_dir= direc + 'logs', batch_size=64, histogram_freq=5,\n",
    "#                                               write_graph=True, write_images=False, write_grads=True)\n",
    "\n",
    "                ################\n",
    "                ##    Model   ##\n",
    "                ################    \n",
    "\n",
    "                model = get_model(N, classes, activation)\n",
    "                if early:\n",
    "                    earlyStopping = EarlyStopping(monitor='val_loss', patience=3, \n",
    "                                                  verbose=0, mode='auto')\n",
    "                    model.fit([dTrain_1, dTrain_2], yTrain, \n",
    "                              batch_size=batch_size, epochs=epochs, \n",
    "                              validation_split=validation_set, verbose=1,\n",
    "                              callbacks=[earlyStopping, tensorboard])\n",
    "                else:\n",
    "                    model.fit([dTrain_1, dTrain_2], yTrain, \n",
    "                              batch_size=batch_size, epochs=epochs, \n",
    "                              validation_split=validation_set, verbose=1,\n",
    "                              callbacks=[tensorboard])\n",
    "\n",
    "                yPred = np.append(yPred, np.argmax(model.predict([dTest_1, dTest_2]), axis=1))\n",
    "\n",
    "                #################\n",
    "                ##  Serialize  ##\n",
    "                #################      \n",
    "\n",
    "                modelDirectory = direc + 'model/'\n",
    "                if not os.path.exists(modelDirectory):\n",
    "                    print('[+] Creando Directorio \\n\\t ->', modelDirectory)\n",
    "                    os.mkdir(modelDirectory)\n",
    "\n",
    "                serialize_model(modelDirectory + str(modelNum), model)\n",
    "                modelNum += 1\n",
    "\n",
    "                del dTrain, dTest, yTrain, yTest, model\n",
    "                break\n",
    "\n",
    "            yPred = np.array([classes[int(i)]  for i in yPred])\n",
    "\n",
    "            # Save Matrix       \n",
    "            print('\\n \\t\\t\\t [+] Saving Results in', filename_exp)\n",
    "            np.save(filename_exp, [yReal, yPred, sReal])\n",
    "            print('*'*30)\n",
    "#             except Exception as e:\n",
    "#                 print('\\t\\t\\t [!] Fatal Error:\\n\\t\\t', str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Obteniendo Filenames\n",
      "[!] Files in Memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14415/451972 [00:00<00:03, 144147.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Permutation applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 451972/451972 [00:02<00:00, 208779.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Loaded Files\n",
      "\t\t [+] Entrenando \n",
      "\t\t\t [!] Early Stopping False \n",
      "\t\t\t [!] Activation tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:00<?, ?it/s]/Users/Carlos/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:299: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "100%|██████████| 320/320 [00:09<00:00, 33.32it/s] \n",
      "100%|██████████| 18/18 [00:00<00:00, 96.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 48 samples\n",
      "Epoch 1/10\n",
      "272/272 [==============================] - 4s - loss: 1.7411 - acc: 0.2463 - val_loss: 1.3027 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 4s - loss: 1.8513 - acc: 0.5441 - val_loss: 1.5469 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 3s - loss: 1.3417 - acc: 0.5993 - val_loss: 2.0892 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 3s - loss: 1.1381 - acc: 0.6360 - val_loss: 3.1592 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 3s - loss: 0.9721 - acc: 0.6949 - val_loss: 3.7245 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 3s - loss: 0.8619 - acc: 0.6912 - val_loss: 3.5613 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 3s - loss: 0.8471 - acc: 0.6985 - val_loss: 2.8165 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 3s - loss: 0.7839 - acc: 0.7059 - val_loss: 2.4262 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 3s - loss: 0.7312 - acc: 0.7390 - val_loss: 2.6847 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 2s - loss: 0.6543 - acc: 0.7574 - val_loss: 3.0202 - val_acc: 0.0000e+00\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "[+] Creando Directorio \n",
      "\t -> ./ResultadosSubClasses/tanh//model/\n",
      "\n",
      " \t\t\t [+] Saving Results in ./ResultadosSubClasses/tanh//1) Red 500\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "print('[+] Obteniendo Filenames')\n",
    "files = np.array(get_files(extraRandom, permutation))\n",
    "YSubClass = []\n",
    "for file, num in files:\n",
    "    YSubClass.append(get_name_with_survey(file))\n",
    "YSubClass = np.array(YSubClass)\n",
    "    \n",
    "NUMBER_OF_POINTS = 500\n",
    "while NUMBER_OF_POINTS <= MAX_NUMBER_OF_POINTS:\n",
    "    \n",
    "    # Create Folder\n",
    "    directory = './Resultados' + NombreCarpeta\n",
    "    if not os.path.exists(directory):\n",
    "        print('[+] Creando Directorio \\n\\t ->', directory)\n",
    "        os.mkdir(directory)\n",
    "    \n",
    "    experiment(directory, files, YSubClass, subclasses, NUMBER_OF_POINTS, n_splits)\n",
    "    NUMBER_OF_POINTS += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 500, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.arange(0, N, 0.01).reshape(-1, N, 1)\n",
    "np.shape(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Concatenate, Conv1D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "N = 500\n",
    "filters = 64\n",
    "filters2 = 32\n",
    "kernel_size = 42\n",
    "kernel_size2 = 42\n",
    "dropout = 0.5\n",
    "validation_set = 0.2\n",
    "epochs = 1\n",
    "batch_size = 10\n",
    "hidden_dims = 128 \n",
    "stride = 2\n",
    "\n",
    "t = np.arange(0, N, 0.01).reshape(-1, N, 1)\n",
    "m = np.arange(0, N, 0.01).reshape(-1, N, 1)\n",
    "\n",
    "aux = int(N/5)\n",
    "t1 = np.arange(0, aux, 0.1).reshape(-1, 50, 1)\n",
    "m1 = np.arange(0, aux, 0.1).reshape(-1, 50, 1)\n",
    "\n",
    "aux = int((len(t)/2))\n",
    "y = [1, 0] * aux\n",
    "y += [0, 1] * (len(t) - aux)\n",
    "y = np.array(y).reshape(-1, 2)\n",
    "\n",
    "conv1 = Conv1D(filters, kernel_size, strides=stride, activation='relu')\n",
    "conv2 = Conv1D(filters2, kernel_size2, strides=stride, activation='relu')\n",
    "\n",
    "# For Time Tower\n",
    "input1 = Input((N, 1))\n",
    "out1 = conv1(input1)\n",
    "out1 = conv2(out1)\n",
    "\n",
    "# For Magnitude Tower\n",
    "input2 = Input((N, 1))\n",
    "out2 = conv1(input2)\n",
    "out2 = conv2(out2)\n",
    "\n",
    "out = Concatenate()([out1, out2])\n",
    "out = Flatten()(out)\n",
    "out = Dropout(dropout)(out)\n",
    "out = Dense(hidden_dims, activation='tanh')(out)   \n",
    "out = Dropout(dropout)(out)\n",
    "out = Dense(2, activation='softmax')(out)   \n",
    "\n",
    "model = Model([input1, input2], out)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.fit([t, m], y, \n",
    "#           batch_size=batch_size, epochs=epochs, \n",
    "#           validation_split=validation_set, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 500, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 500, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 230, 64)       2752        input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 95, 32)        86048       conv1d_3[0][0]                   \n",
      "                                                                   conv1d_3[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 95, 64)        0           conv1d_4[0][0]                   \n",
      "                                                                   conv1d_4[1][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 6080)          0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 6080)          0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           778368      dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             258         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 867,426\n",
      "Trainable params: 867,426\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yPred = []\n",
    "model.predict([t1, m1])\n",
    "yPred = np.append(yPred, np.argmax(model.predict([t1, m1]), axis=1))\n",
    "np.save('./Prueba..npy', yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 500, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 500, 1)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 451, 128)      6528        input_11[0][0]                   \n",
      "                                                                   input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 402, 64)       409664      conv1d_11[0][0]                  \n",
      "                                                                   conv1d_11[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 402, 128)      0           conv1d_12[0][0]                  \n",
      "                                                                   conv1d_12[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 51456)         0           concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 51456)         0           flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 128)           6586496     dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 128)           0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 2)             258         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 7,002,946\n",
      "Trainable params: 7,002,946\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Conv1D(strides=1, padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
